--------------------------------------------------------------------------
[[27387,1],0]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: plasmon-ms

Another transport will be used instead, although this may result in
lower performance.

NOTE: You can disable this warning by setting the MCA parameter
btl_base_warn_component_unused to 0.
--------------------------------------------------------------------------

*************** JDFTx 1.6.0  ***************

Start date and time: Wed Apr  7 17:48:14 2021
Executable jdftx with command-line: -i BN_SCFALL
Running on hosts (process indices):  plasmon-ms.mit.edu (0-19)
Divided in process groups (process indices):  0 (0)  1 (1)  2 (2)  3 (3)  4 (4)  5 (5)  6 (6)  7 (7)  8 (8)  9 (9)  10 (10)  11 (11)  12 (12)  13 (13)  14 (14)  15 (15)  16 (16)  17 (17)  18 (18)  19 (19)
Resource initialization completed at t[s]:      1.74
Run totals: 20 processes, 40 threads, 0 GPUs
Could not open file 'BN_SCFALL' for reading.
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
